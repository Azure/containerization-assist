name: CI Pipeline

on:
  pull_request:
    branches: [ main ]
  push:
    branches: [ main ]
  workflow_dispatch:

# Cancel in-progress runs when a new commit is pushed
concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

permissions:
  contents: read
  issues: write
  pull-requests: write
  statuses: write

jobs:
  # Phase 1: Fast Canary Validation - Lightweight checks that gate everything else
  canary:
    name: Fast Canary Validation
    runs-on: ubuntu-latest
    outputs:
      should-continue: ${{ steps.canary-check.outputs.success }}
      test-mcp: ${{ steps.detect-paths.outputs.test_mcp }}
      test-core: ${{ steps.detect-paths.outputs.test_core }}
      test-cli: ${{ steps.detect-paths.outputs.test_cli }}

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Need full history for diff

    - uses: actions/setup-go@v5
      with:
        go-version: '1.24'

    - name: Install golangci-lint
      run: |
        curl -sSfL https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh | sh -s -- -b $(go env GOPATH)/bin v2.1.6

    - name: Detect changed paths
      id: detect-paths
      run: |
        if [ "${{ github.event_name }}" = "pull_request" ]; then
          # Get changed files
          changed_files=$(git diff --name-only ${{ github.event.pull_request.base.sha }}..HEAD)
          echo "Changed files:"
          echo "$changed_files"

          # Check for MCP changes
          mcp_changes=$(echo "$changed_files" | grep -E "^(pkg/mcp|cmd/mcp-server)" || true)

          # Check for Core changes
          core_changes=$(echo "$changed_files" | grep -E "^(pkg/core|pkg/utils)" || true)

          # Check for CLI changes
          cli_changes=$(echo "$changed_files" | grep -E "^(pkg/ai|pkg/pipeline|cmd/)" || true)

          # Set outputs
          echo "test_mcp=$([ -n "$mcp_changes" ] && echo "true" || echo "false")" >> $GITHUB_OUTPUT
          echo "test_core=$([ -n "$core_changes" ] && echo "true" || echo "false")" >> $GITHUB_OUTPUT
          echo "test_cli=$([ -n "$cli_changes" ] && echo "true" || echo "false")" >> $GITHUB_OUTPUT

          echo "MCP changes: $([ -n "$mcp_changes" ] && echo "yes" || echo "no")"
          echo "Core changes: $([ -n "$core_changes" ] && echo "yes" || echo "no")"
          echo "CLI changes: $([ -n "$cli_changes" ] && echo "yes" || echo "no")"
        else
          # For non-PR events, test everything
          echo "test_mcp=true" >> $GITHUB_OUTPUT
          echo "test_core=true" >> $GITHUB_OUTPUT
          echo "test_cli=true" >> $GITHUB_OUTPUT
          echo "Non-PR event: testing all packages"
        fi

    - name: Fast format check
      id: format-check
      run: |
        echo "🎨 Checking formatting..."
        if [ "${{ github.event_name }}" = "pull_request" ]; then
          # Get changed files (excluding deleted files)
          changed_files=$(git diff --name-only --diff-filter=ACMRTUXB origin/${{ github.base_ref }}...HEAD | grep '\.go$' || true)
          if [ -n "$changed_files" ]; then
            # Filter out files that don't exist (extra safety check)
            existing_files=""
            for file in $changed_files; do
              if [ -f "$file" ]; then
                existing_files="$existing_files $file"
              fi
            done
            
            if [ -n "$existing_files" ]; then
              unformatted=$(gofmt -l $existing_files)
              if [ -n "$unformatted" ]; then
                echo "❌ Formatting issues in:"
                echo "$unformatted"
                echo "success=false" >> $GITHUB_OUTPUT
                exit 1
              fi
            fi
          fi
        else
          # For main branch, check all files
          unformatted=$(gofmt -l .)
          if [ -n "$unformatted" ]; then
            echo "❌ Formatting issues found"
            echo "success=false" >> $GITHUB_OUTPUT
            exit 1
          fi
        fi
        echo "✅ Format check passed"

    - name: Fast syntax check
      run: |
        echo "📦 Checking syntax..."
        # Quick syntax check - just compile without output
        go build -o /dev/null ./...
        echo "✅ Syntax check passed"


    - name: Verify go.mod
      run: |
        echo "📋 Verifying go.mod..."
        if ! go mod verify; then
          echo "❌ go.mod verification failed"
          echo "success=false" >> $GITHUB_OUTPUT
          exit 1
        fi
        echo "✅ go.mod verified"

    - name: Set canary result
      id: canary-check
      run: |
        echo "🎉 Canary validation passed!"
        echo "success=true" >> $GITHUB_OUTPUT

  # Phase 2: Parallel Unit Tests (only run if canary passes)
  unit-test-core:
    name: Unit Tests - Core Packages
    needs: canary
    if: needs.canary.outputs.should-continue == 'true'
    uses: ./.github/workflows/reusable-go-build.yml
    with:
      go-version: '1.24'
      packages: './pkg/core/... ./pkg/pipeline/... ./pkg/utils/... ./pkg/docker/... ./pkg/k8s/... ./pkg/kind/...'
      run-tests: true
      run-race-tests: true
      run-lint: false
      coverage: true
      job-suffix: 'core'

  unit-test-mcp:
    name: Unit Tests - MCP Packages
    needs: canary
    if: needs.canary.outputs.should-continue == 'true'
    uses: ./.github/workflows/reusable-go-build.yml
    with:
      go-version: '1.24'
      packages: './pkg/mcp/...'
      run-tests: true
      run-race-tests: true
      run-lint: false
      coverage: true
      job-suffix: 'mcp'

  unit-test-cli:
    name: Unit Tests - CLI Packages
    needs: canary
    if: needs.canary.outputs.should-continue == 'true'
    uses: ./.github/workflows/reusable-go-build.yml
    with:
      go-version: '1.24'
      packages: './pkg/ai/...'
      run-tests: true
      run-race-tests: true
      run-lint: false
      coverage: true
      job-suffix: 'cli'

  # Phase 3: Build Binaries (parallel with unit tests)
  build-cli:
    name: Build CLI Binary
    needs: canary
    if: needs.canary.outputs.should-continue == 'true'
    uses: ./.github/workflows/reusable-go-build.yml
    with:
      go-version: '1.24'
      packages: './cmd/...'
      run-tests: false
      build-binary: true
      binary-output: 'container-kit'
      binary-main: './main.go'

  build-mcp:
    name: Build MCP Server
    needs: canary
    if: needs.canary.outputs.should-continue == 'true'
    uses: ./.github/workflows/reusable-go-build.yml
    with:
      go-version: '1.24'
      packages: './cmd/mcp-server/...'
      run-tests: false
      build-binary: true
      binary-output: 'container-kit-mcp'
      binary-main: './cmd/mcp-server'

  # Phase 4: Coverage Enforcement & Ratchet (after unit tests)
  coverage-enforcement:
    name: Coverage Enforcement
    runs-on: ubuntu-latest
    needs: [canary, unit-test-core, unit-test-mcp, unit-test-cli]
    if: always() && needs.canary.outputs.should-continue == 'true' && !failure()

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
    - uses: ./.github/actions/coverage-enforcement
      with:
        config-file: '.github/quality-config.json'

  # Phase 5: Security Scanning (parallel with coverage)
  security-scan:
    name: Security Scanning
    runs-on: ubuntu-latest
    needs: [canary, unit-test-core, unit-test-mcp, unit-test-cli]
    if: always() && needs.canary.outputs.should-continue == 'true' && !failure()

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
    - uses: ./.github/actions/security-scan
      with:
        artifact-name: security-reports-${{ github.run_id }}

  # Phase 6: Quality Gates (after coverage and security)
  quality-gates:
    name: Quality Gate Checks
    runs-on: ubuntu-latest
    needs: [canary, unit-test-core, unit-test-mcp, unit-test-cli, coverage-enforcement, security-scan]
    if: always() && needs.canary.outputs.should-continue == 'true' && !failure()
    steps:
    - run: |
        echo "🎯 Quality Gates Summary"
        echo "✅ All unit tests passed"
        echo "✅ All builds completed successfully"
        echo "✅ Coverage enforcement passed"
        echo "✅ Security scanning passed"
        echo "🎉 All quality gates passed!"

  # Phase 5: Integration Tests (after everything else)
  integration-tests:
    runs-on: ubuntu-latest
    name: Integration Tests
    needs: [canary, unit-test-core, unit-test-cli, build-cli]
    if: always() && needs.canary.outputs.should-continue == 'true' && !failure() && needs.canary.outputs.test-cli == 'true'
    strategy:
      max-parallel: 25
      matrix:
        test-repo:
          - Azure-Samples/containerize-and-deploy-Java-app-to-Azure
          # - Azure-Samples/open-liberty-on-aks
          # - Azure/wildfly-container-quickstart
          - Mariemfakhreldein/e-commerce-app
          # - N-Usha/java-on-aks-piggymetrics
          - SaiUpadhyayula/SpringAngularEcommerce
          # - Zaaim-Halim/java-EE-E-Commerce-Web-App
          # - agoncal/agoncal-application-petstore-ee7
          - aws-samples/aws-codedeploy-sample-tomcat
          - chamilad/tomcat-hello-world
          # - colinbut/monolith-enterprise-application
          # - dhruvinrsoni/online-pizza-ordering-system
          # - eNKay2408/Note-Taking
          # - jamesfalkner/jboss-daytrader
          - konveyor-ecosystem/coolstore
          # - kparent/jboss-helloworld
          # - oracle-samples/weblogic-examples
          # - shekhargulati/todoapp-forge
          # - spring-petclinic/spring-petclinic-microservices
        test-number: [1, 2, 3, 4, 5]
      fail-fast: false

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Download CLI binary
        uses: actions/download-artifact@v4
        with:
          name: container-kit-${{ runner.os }}-${{ github.run_id }}
          path: ./

      - name: Run Integration Test
        id: integration-test
        uses: ./.github/actions/integration-test-runner
        with:
          repository: ${{ matrix.test-repo }}
          test-number: ${{ matrix.test-number }}
          binary-name: container-kit
          azure-openai-key: ${{ secrets.AZURE_OPENAI_KEY }}
          azure-openai-endpoint: ${{ secrets.AZURE_OPENAI_ENDPOINT }}
          azure-openai-deployment-id: ${{ secrets.AZURE_OPENAI_DEPLOYMENT_ID }}

      - name: Upload test artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: cc-run-${{ steps.integration-test.outputs.run-name }}
          path: test-repo/artifacts/
          retention-days: 14

  aggregate-integration-matrix-results:
    needs: [integration-tests]
    runs-on: ubuntu-latest
    steps:
      - name: Download all results
        uses: actions/download-artifact@v4
        with:
          pattern: cc-run-*
          path: results

      - name: Integration Test Results Summary
        id: aggregate_results
        run: |
          tests_per_repo=5

          github_summary_table=""
          echo "## Container-Kit Integration Test Results" > $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          github_summary_table+="| Repo | Status | Success Rate | Passed | Failed | Timed Out |
          "
          github_summary_table+="| --- | --- | --- | --- | --- | --- |
          "

          # List repos without numbers
          export REPOS="$(ls results | grep -oP 'cc-run-\K[^#]+' | uniq)"
          echo "Repos: $REPOS"

          success_rates=()

          single_run_pass_threshold=30
          single_run_green_threshold=80

          # Loop through repos
          for repo in $REPOS; do
            echo "Processing results for $repo"

            # Count successes and failures
            success_count=0
            failure_count=0

            # Build detailed results
            passed_tests=""
            failed_tests="" # includes failed and timed out tests
            timedout_test_count=0

            # Loop through test runs
            for ((i=1; i<=$tests_per_repo; i++)); do
              if [ -f "results/cc-run-$repo#$i/result.txt" ]; then
                result=$(cat "results/cc-run-$repo#$i/result.txt")
                if [ "$result" == "success" ]; then
                  success_count=$((success_count + 1))
                  passed_tests+="- Test Run $i ✅\n"
                else
                  if [ "$result" == "timeout" ]; then
                    timedout_test_count=$((timedout_test_count + 1))
                  else
                    failure_count=$((failure_count + 1))
                    failed_tests+="- Test Run $i ❌\n"
                    echo "::warning::Test Run $i Failed"
                  fi
                fi
              else
                echo "::warning::Test Run $repo#$i Result Not Found"
              fi
            done

            success_rate=$((success_count * 100 / tests_per_repo))
            completed_tests=$((success_count + failure_count))

            success_emote=""
            if [ $success_rate -ge $single_run_green_threshold ]; then
              success_emote="✅"
            elif [ $success_rate -ge $single_run_pass_threshold ]; then
              success_emote="⚠️"
            else
              success_emote="❌"
            fi

            # Show results
            echo ""
            echo "===== $repo Test Results ====="
            echo "Total tests: $tests_per_repo"
            echo "Passed: $success_count"
            echo "Failed: $failure_count"
            echo "Timed out: $timedout_test_count"
            echo "Success rate: $success_rate%"

            # Convert the repo name back to GitHub URL format (replace dots with slashes)
            repo_url=$(echo "$repo" | sed 's/\./\//g')
            repo_markdown_link="[$repo_url](https://github.com/$repo_url)"
            github_summary_table+="| $repo_markdown_link | $success_emote | $success_rate% | $success_count | $failure_count | $timedout_test_count |
            "
            echo ""
            success_rates+=("$success_rate")
          done

          pass_threshold=50
          green_threshold=80
          average_success_rate=0
          for rate in "${success_rates[@]}"; do
            average_success_rate=$((average_success_rate + rate))
          done
          average_success_rate=$((average_success_rate / ${#success_rates[@]}))
          pass_rate_message=""
          if [ $average_success_rate -ge $green_threshold ]; then
            pass_rate_message="✅ Average success rate: $average_success_rate% >= $green_threshold%"
          elif [ $average_success_rate -ge $pass_threshold ]; then
            pass_rate_message="⚠️ Average success rate: $average_success_rate% >= $pass_threshold%"
          else
            pass_rate_message="❌ Average success rate: $average_success_rate% < $pass_threshold%"
          fi
          echo "$pass_rate_message"
          echo "" >> $GITHUB_STEP_SUMMARY # add a blank line to separate the summary from the table
          echo "$pass_rate_message" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Output the summary table
          echo "$github_summary_table" >> $GITHUB_STEP_SUMMARY

          #last check to fail the job if the average success rate is below the threshold
          if [ $average_success_rate -lt $pass_threshold ]; then
            echo "Average success rate: $average_success_rate% < $pass_threshold%"
            exit 1
          fi

  # Phase 6: MCP Integration Tests (parallel with integration tests)
  mcp-integration-tests:
    name: MCP Integration Tests
    needs: [canary, unit-test-mcp, build-mcp]
    if: always() && needs.canary.outputs.should-continue == 'true' && !failure() && needs.canary.outputs.test-mcp == 'true'
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4
    - uses: ./.github/actions/mcp-integration-tests
      with:
        test-scenario: 'basic'
        log-level: 'debug'

  # MCP Workflow Integration Tests - Dedicated job for comprehensive workflow testing
  mcp-workflow-tests:
    name: MCP Workflow Integration Tests
    needs: [canary, unit-test-mcp, build-mcp]
    if: always() && needs.canary.outputs.should-continue == 'true' && !failure() && needs.canary.outputs.test-mcp == 'true'
    runs-on: ubuntu-latest

    strategy:
      matrix:
        test-scenario: ["simple-go-service", "all-workflows"]

    steps:
    - uses: actions/checkout@v4
    - uses: ./.github/actions/mcp-integration-tests
      with:
        test-scenario: ${{ matrix.test-scenario }}
        log-level: 'debug'
        timeout: '15m'

  # Phase 7: Testing Dashboard (after all tests complete)
  testing-dashboard:
    name: Testing Dashboard
    runs-on: ubuntu-latest
    needs: [canary, unit-test-core, unit-test-mcp, unit-test-cli, coverage-enforcement, security-scan, quality-gates, mcp-workflow-tests]
    if: always() && needs.canary.outputs.should-continue == 'true'

    steps:
    - name: Generate testing dashboard
      timeout-minutes: 2
      run: |
        echo "📊 Generating fast testing dashboard..."

        # Create a simple testing report using data from previous jobs
        cat > /tmp/testing-report.md << 'EOF'
        # 🧪 Testing Progress Dashboard

        ## Pipeline Status Summary
        | Component | Status |
        |-----------|--------|
        | Canary Validation | ${{ needs.canary.result == 'success' && '✅ Passed' || '❌ Failed' }} |
        | Unit Tests (Core) | ${{ needs.unit-test-core.result == 'success' && '✅ Passed' || needs.unit-test-core.result == 'skipped' && '⏭️ Skipped' || '❌ Failed' }} |
        | Unit Tests (MCP) | ${{ needs.unit-test-mcp.result == 'success' && '✅ Passed' || needs.unit-test-mcp.result == 'skipped' && '⏭️ Skipped' || '❌ Failed' }} |
        | Unit Tests (CLI) | ${{ needs.unit-test-cli.result == 'success' && '✅ Passed' || needs.unit-test-cli.result == 'skipped' && '⏭️ Skipped' || '❌ Failed' }} |
        | Coverage Enforcement | ${{ needs.coverage-enforcement.result == 'success' && '✅ Passed' || needs.coverage-enforcement.result == 'skipped' && '⏭️ Skipped' || '❌ Failed' }} |
        | Security Scanning | ${{ needs.security-scan.result == 'success' && '✅ Passed' || needs.security-scan.result == 'skipped' && '⏭️ Skipped' || '❌ Failed' }} |
        | Quality Gates | ${{ needs.quality-gates.result == 'success' && '✅ Passed' || needs.quality-gates.result == 'skipped' && '⏭️ Skipped' || '❌ Failed' }} |
        | MCP Workflow Tests | ${{ needs.mcp-workflow-tests.result == 'success' && '✅ Passed' || needs.mcp-workflow-tests.result == 'skipped' && '⏭️ Skipped' || '❌ Failed' }} |

        ## 📋 Quick Stats
        - **Workflow**: CI Pipeline
        - **Run ID**: ${{ github.run_id }}
        - **Commit**: ${{ github.sha }}
        - **Branch**: ${{ github.ref_name }}

        ## 🔗 Detailed Results
        For detailed test results, coverage reports, and logs, check the [GitHub Actions run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}).

        ## 📊 Coverage Reports
        Coverage reports are available in the artifacts of this workflow run.

        ---
        *Generated by GitHub Actions*
        EOF

        echo "✅ Fast testing dashboard generated successfully"

    - name: Upload test coverage reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: testing-dashboard-coverage-${{ github.run_id }}
        path: |
          /tmp/coverage-*.out
          coverage.out
        retention-days: 7

    - name: Upload testing report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: testing-report-${{ github.run_id }}
        path: /tmp/testing-report.md
        retention-days: 30

  # Final Phase: Pipeline Summary
  pipeline-summary:
    name: Pipeline Summary
    runs-on: ubuntu-latest
    needs: [canary, coverage-enforcement, security-scan, quality-gates]
    if: always()
    permissions:
      contents: read
      pull-requests: write
      issues: write

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
        ref: ${{ github.event_name == 'pull_request' && github.event.pull_request.head.sha || github.sha }}

    - name: Run architecture analysis
      id: arch-analysis
      uses: ./.github/actions/architecture-check
      with:
        path: 'pkg/mcp'
        config-file: '.github/quality-config.json'

    - name: Generate comprehensive PR status and summary
      uses: ./.github/actions/pr-status-generator
      with:
        github-token: ${{ secrets.GITHUB_TOKEN }}
        run-id: ${{ github.run_id }}
        architecture-score: ${{ steps.arch-analysis.outputs.architecture-score }}
        architecture-status: ${{ steps.arch-analysis.outputs.architecture-status }}
        architecture-details: 'Adapters/Wrappers/Cycles: ${{ steps.arch-analysis.outputs.adapter-count }}/${{ steps.arch-analysis.outputs.wrapper-count }}/${{ steps.arch-analysis.outputs.import-cycles }} | Large files: ${{ steps.arch-analysis.outputs.large-files }} | Complex functions: ${{ steps.arch-analysis.outputs.complex-functions }}'
        quality-results: ${{ needs.quality-gates.result == 'success' && '✅ All quality checks passed' || '⚠️ Quality checks failed or skipped' }}

  # Status Check - Single job to use as required check in branch protection
  # This job will only pass if ALL required jobs pass
  ci-status-check:
    name: CI Status Check
    runs-on: ubuntu-latest
    needs: [canary, unit-test-core, unit-test-mcp, unit-test-cli, build-cli, build-mcp, coverage-enforcement, security-scan, quality-gates]
    if: always()
    steps:
      - name: Check all required jobs
        run: |
          echo "Checking status of all required jobs..."

          # Check each job result
          FAILED_JOBS=""

          # Canary must always pass
          if [ "${{ needs.canary.result }}" != "success" ]; then
            FAILED_JOBS="${FAILED_JOBS}canary "
          fi

          # Build jobs must pass
          if [ "${{ needs.build-cli.result }}" != "success" ]; then
            FAILED_JOBS="${FAILED_JOBS}build-cli "
          fi
          if [ "${{ needs.build-mcp.result }}" != "success" ]; then
            FAILED_JOBS="${FAILED_JOBS}build-mcp "
          fi

          # Unit tests must pass if they ran (not skipped)
          if [ "${{ needs.unit-test-core.result }}" != "success" ] && [ "${{ needs.unit-test-core.result }}" != "skipped" ]; then
            FAILED_JOBS="${FAILED_JOBS}unit-test-core "
          fi
          if [ "${{ needs.unit-test-mcp.result }}" != "success" ] && [ "${{ needs.unit-test-mcp.result }}" != "skipped" ]; then
            FAILED_JOBS="${FAILED_JOBS}unit-test-mcp "
          fi
          if [ "${{ needs.unit-test-cli.result }}" != "success" ] && [ "${{ needs.unit-test-cli.result }}" != "skipped" ]; then
            FAILED_JOBS="${FAILED_JOBS}unit-test-cli "
          fi

          # Coverage must pass if it ran
          if [ "${{ needs.coverage-enforcement.result }}" != "success" ] && [ "${{ needs.coverage-enforcement.result }}" != "skipped" ]; then
            FAILED_JOBS="${FAILED_JOBS}coverage-enforcement "
          fi

          # Security scan must pass if it ran
          if [ "${{ needs.security-scan.result }}" != "success" ] && [ "${{ needs.security-scan.result }}" != "skipped" ]; then
            FAILED_JOBS="${FAILED_JOBS}security-scan "
          fi

          # Quality gates must pass if it ran
          if [ "${{ needs.quality-gates.result }}" != "success" ] && [ "${{ needs.quality-gates.result }}" != "skipped" ]; then
            FAILED_JOBS="${FAILED_JOBS}quality-gates "
          fi

          # Note: Integration tests and dashboard are optional and not checked here

          # Fail if any required jobs failed
          if [ -n "$FAILED_JOBS" ]; then
            echo "❌ The following required jobs failed: $FAILED_JOBS"
            exit 1
          else
            echo "✅ All required CI checks passed!"
          fi
