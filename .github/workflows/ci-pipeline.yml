name: CI Pipeline

on:
  pull_request:
    branches: [ main ]
  push:
    branches: [ main ]
  workflow_dispatch:

# Cancel in-progress runs when a new commit is pushed
concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

permissions:
  contents: read
  issues: write
  pull-requests: write
  statuses: write

jobs:
  # Phase 1: Fast Canary Validation - Lightweight checks that gate everything else
  canary:
    name: Fast Canary Validation
    runs-on: ubuntu-latest
    outputs:
      should-continue: ${{ steps.canary-check.outputs.success }}
      test-mcp: ${{ steps.detect-paths.outputs.test_mcp }}
      test-core: ${{ steps.detect-paths.outputs.test_core }}
      test-cli: ${{ steps.detect-paths.outputs.test_cli }}

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Need full history for diff

    - uses: actions/setup-go@v5
      with:
        go-version: '1.24'

    - name: Install golangci-lint
      run: |
        curl -sSfL https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh | sh -s -- -b $(go env GOPATH)/bin v2.1.6

    - name: Detect changed paths
      id: detect-paths
      run: |
        if [ "${{ github.event_name }}" = "pull_request" ]; then
          # Get changed files
          changed_files=$(git diff --name-only ${{ github.event.pull_request.base.sha }}..HEAD)
          echo "Changed files:"
          echo "$changed_files"

          # Check for MCP changes
          mcp_changes=$(echo "$changed_files" | grep -E "^(pkg/mcp|cmd/mcp-server)" || true)

          # Check for Core changes
          core_changes=$(echo "$changed_files" | grep -E "^(pkg/core|pkg/utils)" || true)

          # Check for CLI changes
          cli_changes=$(echo "$changed_files" | grep -E "^(pkg/ai|pkg/pipeline|cmd/)" || true)

          # Set outputs
          echo "test_mcp=$([ -n "$mcp_changes" ] && echo "true" || echo "false")" >> $GITHUB_OUTPUT
          echo "test_core=$([ -n "$core_changes" ] && echo "true" || echo "false")" >> $GITHUB_OUTPUT
          echo "test_cli=$([ -n "$cli_changes" ] && echo "true" || echo "false")" >> $GITHUB_OUTPUT

          echo "MCP changes: $([ -n "$mcp_changes" ] && echo "yes" || echo "no")"
          echo "Core changes: $([ -n "$core_changes" ] && echo "yes" || echo "no")"
          echo "CLI changes: $([ -n "$cli_changes" ] && echo "yes" || echo "no")"
        else
          # For non-PR events, test everything
          echo "test_mcp=true" >> $GITHUB_OUTPUT
          echo "test_core=true" >> $GITHUB_OUTPUT
          echo "test_cli=true" >> $GITHUB_OUTPUT
          echo "Non-PR event: testing all packages"
        fi

    - name: Fast format check
      id: format-check
      run: |
        echo "🎨 Checking formatting..."
        if [ "${{ github.event_name }}" = "pull_request" ]; then
          # Get changed files (excluding deleted files)
          changed_files=$(git diff --name-only --diff-filter=ACMRTUXB origin/${{ github.base_ref }}...HEAD | grep '\.go$' || true)
          if [ -n "$changed_files" ]; then
            # Filter out files that don't exist (extra safety check)
            existing_files=""
            for file in $changed_files; do
              if [ -f "$file" ]; then
                existing_files="$existing_files $file"
              fi
            done

            if [ -n "$existing_files" ]; then
              unformatted=$(gofmt -l $existing_files)
              if [ -n "$unformatted" ]; then
                echo "❌ Formatting issues in:"
                echo "$unformatted"
                echo "success=false" >> $GITHUB_OUTPUT
                exit 1
              fi
            fi
          fi
        else
          # For main branch, check all files
          unformatted=$(gofmt -l .)
          if [ -n "$unformatted" ]; then
            echo "❌ Formatting issues found"
            echo "success=false" >> $GITHUB_OUTPUT
            exit 1
          fi
        fi
        echo "✅ Format check passed"

    - name: Fast syntax check
      run: |
        echo "📦 Checking syntax..."
        # Quick syntax check - just compile without output
        go build -o /dev/null ./...
        echo "✅ Syntax check passed"


    - name: Verify go.mod
      run: |
        echo "📋 Verifying go.mod..."
        if ! go mod verify; then
          echo "❌ go.mod verification failed"
          echo "success=false" >> $GITHUB_OUTPUT
          exit 1
        fi
        echo "✅ go.mod verified"

    - name: Set canary result
      id: canary-check
      run: |
        echo "🎉 Canary validation passed!"
        echo "success=true" >> $GITHUB_OUTPUT

  # Phase 2: Quality Gates & Architecture Validation (runs early for fast feedback)
  quality-gates:
    name: Quality Gates
    needs: canary
    if: needs.canary.outputs.should-continue == 'true'
    uses: ./.github/workflows/quality-gates-reusable.yml
    with:
      source-ref: ${{ github.event.pull_request.head.sha || github.sha }}
      package-filter: './pkg/mcp/...'

  unified-error-validation:
    name: Error Validation
    needs: canary
    if: needs.canary.outputs.should-continue == 'true'
    uses: ./.github/workflows/unified-error-validation.yml
    with:
      source-ref: ${{ github.event.pull_request.head.sha || github.sha }}
      package-filter: './pkg/mcp/...'

  architecture-validation:
    name: Architecture Analysis
    needs: canary
    if: needs.canary.outputs.should-continue == 'true'
    runs-on: ubuntu-latest
    outputs:
      architecture-score: ${{ steps.arch-check.outputs.architecture-score }}
      architecture-status: ${{ steps.arch-check.outputs.architecture-status }}
      architecture-details: ${{ steps.metrics.outputs.details }}

    steps:
    - uses: actions/checkout@v4
      with:
        ref: ${{ github.event.pull_request.head.sha || github.sha }}

    - uses: actions/setup-go@v5
      with:
        go-version: '1.24'

    - name: Architecture validation
      id: arch-check
      uses: ./.github/actions/architecture-check
      with:
        path: 'pkg/mcp'
        complexity-threshold: '15'
        complexity-budget: '50'
        large-file-threshold: '800'
        large-file-budget: '10'

    - name: Calculate additional metrics
      id: metrics
      run: |
        echo "📊 Calculating additional architecture metrics..."

        # Total lines of code in MCP
        TOTAL_LOC=$(find pkg/mcp -name "*.go" -exec wc -l {} + | awk '{sum+=$1} END {print sum}')

        # Interface definitions count
        INTERFACE_COUNT=$(grep -r "type.*Tool.*interface" pkg/mcp/ 2>/dev/null | grep -v "//\|test" | wc -l || echo "0")

        # Build time
        BUILD_START=$(date +%s)
        if go build -tags mcp ./pkg/mcp/... >/dev/null 2>&1; then
          BUILD_END=$(date +%s)
          BUILD_TIME=$((BUILD_END - BUILD_START))
        else
          BUILD_TIME="FAILED"
        fi

        # Create summary details
        DETAILS="Adapters/Wrappers/Cycles: ${{ steps.arch-check.outputs.adapter-count }}/${{ steps.arch-check.outputs.wrapper-count }}/${{ steps.arch-check.outputs.import-cycles }}"
        DETAILS="$DETAILS | Large files: ${{ steps.arch-check.outputs.large-files }}"
        DETAILS="$DETAILS | Complex functions: ${{ steps.arch-check.outputs.complex-functions }}"
        DETAILS="$DETAILS | Total LOC: $TOTAL_LOC"

        echo "details=$DETAILS" >> $GITHUB_OUTPUT

    - name: Update PR with architecture progress
      if: github.event_name == 'pull_request'
      uses: ./.github/actions/simple-pr-updater
      with:
        github-token: ${{ secrets.GITHUB_TOKEN }}
        phase: 'quality'
        phase-status: 'success'
        phase-details: 'Architecture Score: ${{ steps.arch-check.outputs.architecture-score }}/100'

  # Phase 3: Parallel Unit Tests (only run if canary and quality gates pass)
  unit-test-core:
    name: Unit Tests - Core Packages
    needs: [canary, quality-gates, unified-error-validation, architecture-validation]
    if: needs.canary.outputs.should-continue == 'true' && (needs.quality-gates.outputs.passed == 'true' || needs.quality-gates.result == 'success') && (needs.unified-error-validation.outputs.passed == 'true' || needs.unified-error-validation.result == 'success')
    uses: ./.github/workflows/reusable-go-build.yml
    with:
      go-version: '1.24'
      packages: './pkg/core/... ./pkg/pipeline/... ./pkg/utils/... ./pkg/docker/... ./pkg/k8s/... ./pkg/kind/...'
      run-tests: true
      run-race-tests: true
      run-lint: false
      coverage: true
      job-suffix: 'core'

  update-test-progress:
    name: Update Test Progress
    needs: [unit-test-core, unit-test-mcp, unit-test-cli]
    if: always() && github.event_name == 'pull_request'
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - name: Update PR with test progress
      uses: ./.github/actions/simple-pr-updater
      with:
        github-token: ${{ secrets.GITHUB_TOKEN }}
        phase: 'tests'
        phase-status: ${{ 
          (needs.unit-test-core.result == 'success' || needs.unit-test-core.result == 'skipped') && 
          (needs.unit-test-mcp.result == 'success' || needs.unit-test-mcp.result == 'skipped') && 
          (needs.unit-test-cli.result == 'success' || needs.unit-test-cli.result == 'skipped') && 'success' || 'failure' }}
        phase-details: 'Core: ${{ needs.unit-test-core.result }} | MCP: ${{ needs.unit-test-mcp.result }} | CLI: ${{ needs.unit-test-cli.result }}'

  unit-test-mcp:
    name: Unit Tests - MCP Packages
    needs: [canary, quality-gates, unified-error-validation, architecture-validation]
    if: needs.canary.outputs.should-continue == 'true' && (needs.quality-gates.outputs.passed == 'true' || needs.quality-gates.result == 'success') && (needs.unified-error-validation.outputs.passed == 'true' || needs.unified-error-validation.result == 'success')
    uses: ./.github/workflows/reusable-go-build.yml
    with:
      go-version: '1.24'
      packages: './pkg/mcp/...'
      run-tests: true
      run-race-tests: true
      run-lint: false
      coverage: true
      job-suffix: 'mcp'

  unit-test-cli:
    name: Unit Tests - CLI Packages
    needs: [canary, quality-gates, unified-error-validation, architecture-validation]
    if: needs.canary.outputs.should-continue == 'true' && needs.quality-gates.outputs.passed == 'true' && needs.unified-error-validation.outputs.passed == 'true'
    uses: ./.github/workflows/reusable-go-build.yml
    with:
      go-version: '1.24'
      packages: './pkg/ai/...'
      run-tests: true
      run-race-tests: true
      run-lint: false
      coverage: true
      job-suffix: 'cli'

  # Phase 4: Build Binaries (parallel with unit tests)
  build-cli:
    name: Build CLI Binary
    needs: [canary, quality-gates, unified-error-validation, architecture-validation]
    if: needs.canary.outputs.should-continue == 'true' && needs.quality-gates.outputs.passed == 'true' && needs.unified-error-validation.outputs.passed == 'true'
    uses: ./.github/workflows/reusable-go-build.yml
    with:
      go-version: '1.24'
      packages: './cmd/...'
      run-tests: false
      build-binary: true
      binary-output: 'container-kit'
      binary-main: './main.go'

  build-mcp:
    name: Build MCP Server
    needs: [canary, quality-gates, unified-error-validation, architecture-validation]
    if: needs.canary.outputs.should-continue == 'true' && needs.quality-gates.outputs.passed == 'true' && needs.unified-error-validation.outputs.passed == 'true'
    uses: ./.github/workflows/reusable-go-build.yml
    with:
      go-version: '1.24'
      packages: './cmd/mcp-server/...'
      run-tests: false
      build-binary: true
      binary-output: 'container-kit-mcp'
      binary-main: './cmd/mcp-server'

  # Phase 5: RichError Boundary Compliance (after unit tests)
  rich-error-compliance:
    name: RichError Boundary Compliance
    runs-on: ubuntu-latest
    needs: [canary, quality-gates, unit-test-core, unit-test-mcp, unit-test-cli]
    if: always() && needs.canary.outputs.should-continue == 'true' && needs.quality-gates.outputs.passed == 'true' && !failure()

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - uses: actions/setup-go@v5
      with:
        go-version: '1.24'
        cache: true

    - name: Build and run rich-audit
      run: |
        echo "🔍 Running RichError boundary compliance check..."
        go build -o rich-audit ./cmd/rich-audit

        # Audit critical packages
        ./rich-audit -package ./pkg/mcp/core -fail=true
        ./rich-audit -package ./pkg/mcp/common -fail=true
        ./rich-audit -package ./pkg/mcp/internal/transport -fail=true

        echo "✅ RichError compliance check passed"

  # Phase 6: Coverage Enforcement & Ratchet (after unit tests)
  coverage-enforcement:
    name: Coverage Enforcement
    runs-on: ubuntu-latest
    needs: [canary, quality-gates, unit-test-core, unit-test-mcp, unit-test-cli]
    if: always() && needs.canary.outputs.should-continue == 'true' && needs.quality-gates.outputs.passed == 'true' && !failure()

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
    - uses: ./.github/actions/coverage-enforcement
      with:
        config-file: '.github/quality-config.json'

  # Phase 7: Security Scanning (parallel with coverage)
  security-scan:
    name: Security Scanning
    runs-on: ubuntu-latest
    needs: [canary, quality-gates, unit-test-core, unit-test-mcp, unit-test-cli]
    if: always() && needs.canary.outputs.should-continue == 'true' && needs.quality-gates.outputs.passed == 'true' && !failure()

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
    - uses: ./.github/actions/security-scan
      with:
        artifact-name: security-reports-${{ github.run_id }}

  # Phase 8: Integration Tests (PRESERVED - after everything else)
  integration-tests:
    runs-on: ubuntu-latest
    name: Integration Tests
    needs: [canary, quality-gates, unit-test-core, unit-test-cli, build-cli]
    if: always() && needs.canary.outputs.should-continue == 'true' && needs.quality-gates.outputs.passed == 'true' && !failure() && needs.canary.outputs.test-cli == 'true'
    strategy:
      max-parallel: 25
      matrix:
        test-repo:
          - Azure-Samples/containerize-and-deploy-Java-app-to-Azure
          # - Azure-Samples/open-liberty-on-aks
          # - Azure/wildfly-container-quickstart
          - Mariemfakhreldein/e-commerce-app
          # - N-Usha/java-on-aks-piggymetrics
          - SaiUpadhyayula/SpringAngularEcommerce
          # - Zaaim-Halim/java-EE-E-Commerce-Web-App
          # - agoncal/agoncal-application-petstore-ee7
          - aws-samples/aws-codedeploy-sample-tomcat
          - chamilad/tomcat-hello-world
          # - colinbut/monolith-enterprise-application
          # - dhruvinrsoni/online-pizza-ordering-system
          # - eNKay2408/Note-Taking
          # - jamesfalkner/jboss-daytrader
          - konveyor-ecosystem/coolstore
          # - kparent/jboss-helloworld
          # - oracle-samples/weblogic-examples
          # - shekhargulati/todoapp-forge
          # - spring-petclinic/spring-petclinic-microservices
        test-number: [1, 2, 3, 4, 5]
      fail-fast: false

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Download CLI binary
        uses: actions/download-artifact@v4
        with:
          name: container-kit-${{ runner.os }}-${{ github.run_id }}
          path: ./

      - name: Run Integration Test
        id: integration-test
        uses: ./.github/actions/integration-test-runner
        with:
          repository: ${{ matrix.test-repo }}
          test-number: ${{ matrix.test-number }}
          binary-name: container-kit
          azure-openai-key: ${{ secrets.AZURE_OPENAI_KEY }}
          azure-openai-endpoint: ${{ secrets.AZURE_OPENAI_ENDPOINT }}
          azure-openai-deployment-id: ${{ secrets.AZURE_OPENAI_DEPLOYMENT_ID }}

      - name: Upload test artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: cc-run-${{ steps.integration-test.outputs.run-name }}
          path: test-repo/artifacts/
          retention-days: 14

  aggregate-integration-matrix-results:
    needs: [integration-tests]
    runs-on: ubuntu-latest
    steps:
      - name: Download all results
        uses: actions/download-artifact@v4
        with:
          pattern: cc-run-*
          path: results

      - name: Integration Test Results Summary
        id: aggregate_results
        run: |
          tests_per_repo=5

          github_summary_table=""
          echo "## Container-Kit Integration Test Results" > $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          github_summary_table+="| Repo | Status | Success Rate | Passed | Failed | Timed Out |
          "
          github_summary_table+="| --- | --- | --- | --- | --- | --- |
          "

          # List repos without numbers
          export REPOS="$(ls results | grep -oP 'cc-run-\K[^#]+' | uniq)"
          echo "Repos: $REPOS"

          success_rates=()

          single_run_pass_threshold=30
          single_run_green_threshold=80

          # Loop through repos
          for repo in $REPOS; do
            echo "Processing results for $repo"

            # Count successes and failures
            success_count=0
            failure_count=0

            # Build detailed results
            passed_tests=""
            failed_tests="" # includes failed and timed out tests
            timedout_test_count=0

            # Loop through test runs
            for ((i=1; i<=$tests_per_repo; i++)); do
              if [ -f "results/cc-run-$repo#$i/result.txt" ]; then
                result=$(cat "results/cc-run-$repo#$i/result.txt")
                if [ "$result" == "success" ]; then
                  success_count=$((success_count + 1))
                  passed_tests+="- Test Run $i ✅\n"
                else
                  if [ "$result" == "timeout" ]; then
                    timedout_test_count=$((timedout_test_count + 1))
                  else
                    failure_count=$((failure_count + 1))
                    failed_tests+="- Test Run $i ❌\n"
                    echo "::warning::Test Run $i Failed"
                  fi
                fi
              else
                echo "::warning::Test Run $repo#$i Result Not Found"
              fi
            done

            success_rate=$((success_count * 100 / tests_per_repo))
            completed_tests=$((success_count + failure_count))

            success_emote=""
            if [ $success_rate -ge $single_run_green_threshold ]; then
              success_emote="✅"
            elif [ $success_rate -ge $single_run_pass_threshold ]; then
              success_emote="⚠️"
            else
              success_emote="❌"
            fi

            # Show results
            echo ""
            echo "===== $repo Test Results ====="
            echo "Total tests: $tests_per_repo"
            echo "Passed: $success_count"
            echo "Failed: $failure_count"
            echo "Timed out: $timedout_test_count"
            echo "Success rate: $success_rate%"

            # Convert the repo name back to GitHub URL format (replace dots with slashes)
            repo_url=$(echo "$repo" | sed 's/\./\//g')
            repo_markdown_link="[$repo_url](https://github.com/$repo_url)"
            github_summary_table+="| $repo_markdown_link | $success_emote | $success_rate% | $success_count | $failure_count | $timedout_test_count |
            "
            echo ""
            success_rates+=("$success_rate")
          done

          pass_threshold=50
          green_threshold=80
          average_success_rate=0
          for rate in "${success_rates[@]}"; do
            average_success_rate=$((average_success_rate + rate))
          done
          average_success_rate=$((average_success_rate / ${#success_rates[@]}))
          pass_rate_message=""
          if [ $average_success_rate -ge $green_threshold ]; then
            pass_rate_message="✅ Average success rate: $average_success_rate% >= $green_threshold%"
          elif [ $average_success_rate -ge $pass_threshold ]; then
            pass_rate_message="⚠️ Average success rate: $average_success_rate% >= $pass_threshold%"
          else
            pass_rate_message="❌ Average success rate: $average_success_rate% < $pass_threshold%"
          fi
          echo "$pass_rate_message"
          echo "" >> $GITHUB_STEP_SUMMARY # add a blank line to separate the summary from the table
          echo "$pass_rate_message" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Output the summary table
          echo "$github_summary_table" >> $GITHUB_STEP_SUMMARY

          #last check to fail the job if the average success rate is below the threshold
          if [ $average_success_rate -lt $pass_threshold ]; then
            echo "Average success rate: $average_success_rate% < $pass_threshold%"
            exit 1
          fi

  # Phase 6: MCP Integration Tests (parallel with integration tests)
  mcp-integration-tests:
    name: MCP Integration Tests
    needs: [canary, unit-test-mcp, build-mcp]
    if: always() && needs.canary.outputs.should-continue == 'true' && !failure() && needs.canary.outputs.test-mcp == 'true'
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4
    - uses: actions/setup-go@v5
      with:
        go-version: '1.24'
    - name: Run Integration Tests
      run: make test-integration
    - uses: ./.github/actions/mcp-integration-tests
      with:
        test-scenario: 'basic'
        log-level: 'debug'

  # MCP Workflow Integration Tests - Dedicated job for comprehensive workflow testing
  mcp-workflow-tests:
    name: MCP Workflow Integration Tests
    needs: [canary, unit-test-mcp, build-mcp]
    if: always() && needs.canary.outputs.should-continue == 'true' && !failure() && needs.canary.outputs.test-mcp == 'true'
    runs-on: ubuntu-latest

    strategy:
      matrix:
        test-scenario: ["simple-go-service", "all-workflows"]

    steps:
    - uses: actions/checkout@v4
    - uses: actions/setup-go@v5
      with:
        go-version: '1.24'
    - uses: ./.github/actions/mcp-integration-tests
      with:
        test-scenario: ${{ matrix.test-scenario }}
        log-level: 'debug'
        timeout: '15m'

  # GAMMA Workstream: E2E Tests (only on main branch)
  mcp-e2e-tests:
    name: MCP E2E Tests
    needs: [canary, unit-test-mcp, build-mcp]
    if: always() && needs.canary.outputs.should-continue == 'true' && !failure() && needs.canary.outputs.test-mcp == 'true' && github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4
    - uses: actions/setup-go@v5
      with:
        go-version: '1.24'
    - name: Run E2E Tests
      run: make test-e2e

  # GAMMA Workstream: Performance Tests
  mcp-performance-tests:
    name: MCP Performance Tests
    needs: [canary, unit-test-mcp, build-mcp]
    if: always() && needs.canary.outputs.should-continue == 'true' && !failure() && needs.canary.outputs.test-mcp == 'true' && github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4
    - uses: actions/setup-go@v5
      with:
        go-version: '1.24'
    - name: Run Performance Benchmarks
      run: make test-performance

  # Phase 7: Testing Dashboard (after all tests complete)
  testing-dashboard:
    name: Testing Dashboard
    runs-on: ubuntu-latest
    needs: [canary, unit-test-core, unit-test-mcp, unit-test-cli, coverage-enforcement, security-scan, quality-gates, mcp-workflow-tests, mcp-e2e-tests, mcp-performance-tests]
    if: always() && needs.canary.outputs.should-continue == 'true'

    steps:
    - name: Generate testing dashboard
      timeout-minutes: 2
      run: |
        echo "📊 Generating fast testing dashboard..."

        # Create a simple testing report using data from previous jobs
        cat > /tmp/testing-report.md << 'EOF'
        # 🧪 Testing Progress Dashboard

        ## Pipeline Status Summary
        | Component | Status |
        |-----------|--------|
        | Canary Validation | ${{ needs.canary.result == 'success' && '✅ Passed' || '❌ Failed' }} |
        | Quality Gates | ${{ needs.quality-gates.result == 'success' && '✅ Passed' || needs.quality-gates.result == 'skipped' && '⏭️ Skipped' || '❌ Failed' }} |
        | Unit Tests (Core) | ${{ needs.unit-test-core.result == 'success' && '✅ Passed' || needs.unit-test-core.result == 'skipped' && '⏭️ Skipped' || '❌ Failed' }} |
        | Unit Tests (MCP) | ${{ needs.unit-test-mcp.result == 'success' && '✅ Passed' || needs.unit-test-mcp.result == 'skipped' && '⏭️ Skipped' || '❌ Failed' }} |
        | Unit Tests (CLI) | ${{ needs.unit-test-cli.result == 'success' && '✅ Passed' || needs.unit-test-cli.result == 'skipped' && '⏭️ Skipped' || '❌ Failed' }} |
        | Coverage Enforcement | ${{ needs.coverage-enforcement.result == 'success' && '✅ Passed' || needs.coverage-enforcement.result == 'skipped' && '⏭️ Skipped' || '❌ Failed' }} |
        | Security Scanning | ${{ needs.security-scan.result == 'success' && '✅ Passed' || needs.security-scan.result == 'skipped' && '⏭️ Skipped' || '❌ Failed' }} |
        | MCP Workflow Tests | ${{ needs.mcp-workflow-tests.result == 'success' && '✅ Passed' || needs.mcp-workflow-tests.result == 'skipped' && '⏭️ Skipped' || '❌ Failed' }} |
        | MCP E2E Tests | ${{ needs.mcp-e2e-tests.result == 'success' && '✅ Passed' || needs.mcp-e2e-tests.result == 'skipped' && '⏭️ Skipped' || '❌ Failed' }} |
        | MCP Performance Tests | ${{ needs.mcp-performance-tests.result == 'success' && '✅ Passed' || needs.mcp-performance-tests.result == 'skipped' && '⏭️ Skipped' || '❌ Failed' }} |

        ## 📋 Quick Stats
        - **Workflow**: CI Pipeline
        - **Run ID**: ${{ github.run_id }}
        - **Commit**: ${{ github.sha }}
        - **Branch**: ${{ github.ref_name }}

        ## 🔗 Detailed Results
        For detailed test results, coverage reports, and logs, check the [GitHub Actions run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}).

        ## 📊 Coverage Reports
        Coverage reports are available in the artifacts of this workflow run.

        ---
        *Generated by GitHub Actions*
        EOF

        echo "✅ Fast testing dashboard generated successfully"

    - name: Upload test coverage reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: testing-dashboard-coverage-${{ github.run_id }}
        path: |
          /tmp/coverage-*.out
          coverage.out
        retention-days: 7

    - name: Upload testing report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: testing-report-${{ github.run_id }}
        path: /tmp/testing-report.md
        retention-days: 30

  # Final Phase: Pipeline Summary
  pipeline-summary:
    name: Pipeline Summary
    runs-on: ubuntu-latest
    needs: [canary, quality-gates, unit-test-core, unit-test-mcp, unit-test-cli, coverage-enforcement, security-scan, integration-tests, aggregate-integration-matrix-results, mcp-integration-tests, mcp-workflow-tests, mcp-e2e-tests, mcp-performance-tests]
    if: always()
    permissions:
      contents: read
      pull-requests: write
      issues: write

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
        ref: ${{ github.event_name == 'pull_request' && github.event.pull_request.head.sha || github.sha }}

    - name: Calculate pipeline metrics
      id: metrics
      shell: bash
      run: |
        # Calculate canary time (assume canary is fast, ~30s)
        CANARY_TIME="~30s"

        # Calculate total pipeline time (rough estimate based on job statuses)
        PIPELINE_START="${{ github.event.pull_request.created_at || github.event.head_commit.timestamp }}"
        CURRENT_TIME=$(date -u +"%Y-%m-%dT%H:%M:%SZ")

        # Export metrics
        echo "canary-time=$CANARY_TIME" >> $GITHUB_OUTPUT
        echo "total-time=~${GITHUB_RUN_NUMBER}m" >> $GITHUB_OUTPUT

    - name: Run architecture analysis
      id: arch-analysis
      uses: ./.github/actions/architecture-check
      with:
        path: 'pkg/mcp'
        config-file: '.github/quality-config.json'

    - name: Gather test results
      id: test-summary
      shell: bash
      run: |
        # Compile MCP test status
        MCP_STATUS=""
        if [ "${{ needs.mcp-integration-tests.result }}" = "success" ]; then
          MCP_STATUS="Integration: ✅"
        elif [ "${{ needs.mcp-integration-tests.result }}" = "skipped" ]; then
          MCP_STATUS="Integration: ⏭️"
        else
          MCP_STATUS="Integration: ❌"
        fi

        if [ "${{ needs.mcp-e2e-tests.result }}" = "success" ]; then
          MCP_STATUS="$MCP_STATUS | E2E: ✅"
        elif [ "${{ needs.mcp-e2e-tests.result }}" = "skipped" ]; then
          MCP_STATUS="$MCP_STATUS | E2E: ⏭️"
        else
          MCP_STATUS="$MCP_STATUS | E2E: ❌"
        fi

        if [ "${{ needs.mcp-performance-tests.result }}" = "success" ]; then
          MCP_STATUS="$MCP_STATUS | Performance: ✅"
        elif [ "${{ needs.mcp-performance-tests.result }}" = "skipped" ]; then
          MCP_STATUS="$MCP_STATUS | Performance: ⏭️"
        else
          MCP_STATUS="$MCP_STATUS | Performance: ❌"
        fi

        echo "mcp-tests-status=$MCP_STATUS" >> $GITHUB_OUTPUT

    - name: Generate comprehensive PR status and summary
      uses: ./.github/actions/pr-status-generator
      with:
        github-token: ${{ secrets.GITHUB_TOKEN }}
        run-id: ${{ github.run_id }}
        architecture-score: ${{ needs.architecture-validation.outputs.architecture-score }}
        architecture-status: ${{ needs.architecture-validation.outputs.architecture-status }}
        architecture-details: ${{ needs.architecture-validation.outputs.architecture-details }}
        quality-results: ${{ needs.quality-gates.result == 'success' && '✅ All quality checks passed' || '⚠️ Quality checks failed or skipped' }}
        canary-time: ${{ steps.metrics.outputs.canary-time }}
        total-time: ${{ steps.metrics.outputs.total-time }}
        mcp-tests-status: ${{ steps.test-summary.outputs.mcp-tests-status }}
        enable-charts: 'true'
        baseline-coverage: '80'
        baseline-performance: '300μs'

  # Status Check - Single job to use as required check in branch protection
  # This job will only pass if ALL required jobs pass
  ci-status-check:
    name: CI Status Check
    runs-on: ubuntu-latest
    needs: [canary, unit-test-core, unit-test-mcp, unit-test-cli, build-cli, build-mcp, coverage-enforcement, security-scan, quality-gates, unified-error-validation, architecture-validation]
    if: always()
    steps:
      - name: Check all required jobs
        run: |
          echo "Checking status of all required jobs..."

          # Check each job result
          FAILED_JOBS=""

          # Canary must always pass
          if [ "${{ needs.canary.result }}" != "success" ]; then
            FAILED_JOBS="${FAILED_JOBS}canary "
          fi

          # Build jobs must pass (or be skipped due to quality gate failures)
          if [ "${{ needs.build-cli.result }}" != "success" ] && [ "${{ needs.build-cli.result }}" != "skipped" ]; then
            FAILED_JOBS="${FAILED_JOBS}build-cli "
          fi
          if [ "${{ needs.build-mcp.result }}" != "success" ] && [ "${{ needs.build-mcp.result }}" != "skipped" ]; then
            FAILED_JOBS="${FAILED_JOBS}build-mcp "
          fi

          # Unit tests must pass if they ran (not skipped)
          if [ "${{ needs.unit-test-core.result }}" != "success" ] && [ "${{ needs.unit-test-core.result }}" != "skipped" ]; then
            FAILED_JOBS="${FAILED_JOBS}unit-test-core "
          fi
          if [ "${{ needs.unit-test-mcp.result }}" != "success" ] && [ "${{ needs.unit-test-mcp.result }}" != "skipped" ]; then
            FAILED_JOBS="${FAILED_JOBS}unit-test-mcp "
          fi
          if [ "${{ needs.unit-test-cli.result }}" != "success" ] && [ "${{ needs.unit-test-cli.result }}" != "skipped" ]; then
            FAILED_JOBS="${FAILED_JOBS}unit-test-cli "
          fi

          # Coverage must pass if it ran
          if [ "${{ needs.coverage-enforcement.result }}" != "success" ] && [ "${{ needs.coverage-enforcement.result }}" != "skipped" ]; then
            FAILED_JOBS="${FAILED_JOBS}coverage-enforcement "
          fi

          # Security scan must pass if it ran
          if [ "${{ needs.security-scan.result }}" != "success" ] && [ "${{ needs.security-scan.result }}" != "skipped" ]; then
            FAILED_JOBS="${FAILED_JOBS}security-scan "
          fi

          # Quality gates must pass if it ran
          if [ "${{ needs.quality-gates.result }}" != "success" ] && [ "${{ needs.quality-gates.result }}" != "skipped" ]; then
            FAILED_JOBS="${FAILED_JOBS}quality-gates "
          fi

          # Error validation must pass if it ran
          if [ "${{ needs.unified-error-validation.result }}" != "success" ] && [ "${{ needs.unified-error-validation.result }}" != "skipped" ]; then
            FAILED_JOBS="${FAILED_JOBS}unified-error-validation "
          fi

          # Architecture validation must pass if it ran
          if [ "${{ needs.architecture-validation.result }}" != "success" ] && [ "${{ needs.architecture-validation.result }}" != "skipped" ]; then
            FAILED_JOBS="${FAILED_JOBS}architecture-validation "
          fi

          # Note: Integration tests and dashboard are optional and not checked here

          # Fail if any required jobs failed
          if [ -n "$FAILED_JOBS" ]; then
            echo "❌ The following required jobs failed: $FAILED_JOBS"
            exit 1
          else
            echo "✅ All required CI checks passed!"
          fi
