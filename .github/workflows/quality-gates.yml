name: Quality Gates

on:
  pull_request:
    branches: [ main ]
  push:
    branches: [ main ]

permissions:
  contents: read
  pull-requests: write
  issues: write
  statuses: write

jobs:
  quality-checks:
    name: Quality Gate Checks
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Need full history for comparisons

    - name: Set up Go
      uses: actions/setup-go@v5
      with:
        go-version: '1.24'

    - name: Cache Go modules
      uses: actions/cache@v4
      with:
        path: ~/go/pkg/mod
        key: ${{ runner.os }}-go-${{ hashFiles('**/go.sum') }}
        restore-keys: |
          ${{ runner.os }}-go-

    - name: Install dependencies
      run: go mod download

    # Run interface validation
    - name: Validate Interfaces
      id: interfaces
      run: |
        echo "ðŸ” Validating interface compliance..."
        go run tools/validate-interfaces/main.go --metrics --metrics-output interface-metrics.json --error-budget 10

        # Extract key metrics
        ADOPTION_RATE=$(jq '.overview.adoption_rate' interface-metrics.json)
        COMPLIANCE=$(jq '.overview.overall_compliance' interface-metrics.json)
        ERRORS=$(jq '.compliance_report.interface_errors | length' interface-metrics.json)

        echo "adoption_rate=$ADOPTION_RATE" >> $GITHUB_OUTPUT
        echo "compliance=$COMPLIANCE" >> $GITHUB_OUTPUT
        echo "errors=$ERRORS" >> $GITHUB_OUTPUT

        # Fail if there are interface errors
        if [ "$ERRORS" -gt 0 ]; then
          echo "âŒ Found $ERRORS interface validation errors"
          jq '.compliance_report.interface_errors' interface-metrics.json
          exit 1
        fi

    # Run quality dashboard
    - name: Generate Quality Metrics
      id: quality
      run: |
        echo "ðŸ“Š Generating quality metrics..."
        go run tools/quality-dashboard/main.go \
          -output quality-metrics.json \
          -format json

        # Extract metrics
        ERROR_HANDLING=$(jq '.error_handling.adoption_rate' quality-metrics.json)
        TEST_COVERAGE=$(jq '.test_coverage.overall_coverage' quality-metrics.json)
        BUILD_TIME=$(jq -r '.build_metrics.build_time' quality-metrics.json)
        EMPTY_DIRS=$(jq '.directory_structure.empty_directories' quality-metrics.json)
        TODOS=$(jq '.code_quality.todo_comments' quality-metrics.json)

        echo "error_handling=$ERROR_HANDLING" >> $GITHUB_OUTPUT
        echo "test_coverage=$TEST_COVERAGE" >> $GITHUB_OUTPUT
        echo "build_time=$BUILD_TIME" >> $GITHUB_OUTPUT
        echo "empty_dirs=$EMPTY_DIRS" >> $GITHUB_OUTPUT
        echo "todos=$TODOS" >> $GITHUB_OUTPUT

    # Quality gate checks
    - name: Check Quality Gates
      id: gates
      run: |
        echo "ðŸš¦ Checking quality gates..."

        # Define thresholds (with error budgets)
        MIN_ERROR_HANDLING=60.0       # Target 60% RichError adoption
        MIN_ERROR_HANDLING_BUDGET=30.0 # Allow down to 30% during transition
        MIN_TEST_COVERAGE=50.0        # Target 50% test coverage
        MIN_TEST_COVERAGE_BUDGET=10.0 # Allow down to 10% during development
        MAX_BUILD_TIME=300            # Maximum 5 minutes build time
        MAX_EMPTY_DIRS=5              # Maximum 5 empty directories
        WARN_TODOS=50                 # Warning if more than 50 TODOs

        # Check gates
        GATES_PASSED=true
        WARNINGS=""

        # Error handling adoption (with budget)
        if (( $(echo "${{ steps.quality.outputs.error_handling }} < $MIN_ERROR_HANDLING_BUDGET" | bc -l) )); then
          echo "âŒ Error handling adoption (${{ steps.quality.outputs.error_handling }}%) below minimum budget ($MIN_ERROR_HANDLING_BUDGET%)"
          GATES_PASSED=false
        elif (( $(echo "${{ steps.quality.outputs.error_handling }} < $MIN_ERROR_HANDLING" | bc -l) )); then
          echo "âš ï¸  Error handling adoption (${{ steps.quality.outputs.error_handling }}%) below target ($MIN_ERROR_HANDLING%) but within budget"
          WARNINGS="${WARNINGS}\\n- Improve error handling adoption"
        fi

        # Test coverage (with budget)
        if (( $(echo "${{ steps.quality.outputs.test_coverage }} < $MIN_TEST_COVERAGE_BUDGET" | bc -l) )); then
          echo "âŒ Test coverage (${{ steps.quality.outputs.test_coverage }}%) below minimum budget ($MIN_TEST_COVERAGE_BUDGET%)"
          GATES_PASSED=false
        elif (( $(echo "${{ steps.quality.outputs.test_coverage }} < $MIN_TEST_COVERAGE" | bc -l) )); then
          echo "âš ï¸  Test coverage (${{ steps.quality.outputs.test_coverage }}%) below target ($MIN_TEST_COVERAGE%) but within budget"
          WARNINGS="${WARNINGS}\\n- Improve test coverage"
        fi

        # Empty directories
        if [ "${{ steps.quality.outputs.empty_dirs }}" -gt $MAX_EMPTY_DIRS ]; then
          echo "âš ï¸  Warning: ${{ steps.quality.outputs.empty_dirs }} empty directories found (max: $MAX_EMPTY_DIRS)"
          WARNINGS="${WARNINGS}\\n- Clean up empty directories"
        fi

        # TODO comments
        if [ "${{ steps.quality.outputs.todos }}" -gt $WARN_TODOS ]; then
          echo "âš ï¸  Warning: ${{ steps.quality.outputs.todos }} TODO comments found (warning threshold: $WARN_TODOS)"
          WARNINGS="${WARNINGS}\\n- Address TODO comments"
        fi

        echo "gates_passed=$GATES_PASSED" >> $GITHUB_OUTPUT
        echo "warnings=$WARNINGS" >> $GITHUB_OUTPUT

        if [ "$GATES_PASSED" = "false" ]; then
          exit 1
        fi

    # Compare with base branch (PR only)
    - name: Compare with Base Branch
      if: github.event_name == 'pull_request'
      id: compare
      run: |
        echo "ðŸ“ˆ Comparing with base branch..."

        # Stash any uncommitted changes
        git stash push -m "temp stash for base comparison" || true

        # Checkout base branch
        git checkout ${{ github.event.pull_request.base.sha }}

        # Generate base metrics
        go run tools/quality-dashboard/main.go \
          -output base-quality-metrics.json \
          -format json || true

        # Switch back
        git checkout ${{ github.event.pull_request.head.sha }}

        # Restore stashed changes
        git stash pop || true

        # Compare if base metrics exist
        if [ -f base-quality-metrics.json ]; then
          BASE_ERROR=$(jq '.error_handling.adoption_rate' base-quality-metrics.json)
          BASE_COVERAGE=$(jq '.test_coverage.overall_coverage' base-quality-metrics.json)

          CURRENT_ERROR=${{ steps.quality.outputs.error_handling }}
          CURRENT_COVERAGE=${{ steps.quality.outputs.test_coverage }}

          ERROR_DIFF=$(echo "$CURRENT_ERROR - $BASE_ERROR" | bc -l)
          COVERAGE_DIFF=$(echo "$CURRENT_COVERAGE - $BASE_COVERAGE" | bc -l)

          echo "error_diff=$ERROR_DIFF" >> $GITHUB_OUTPUT
          echo "coverage_diff=$COVERAGE_DIFF" >> $GITHUB_OUTPUT

          # Check for regression
          if (( $(echo "$ERROR_DIFF < -5" | bc -l) )); then
            echo "âŒ Error handling adoption regressed by $ERROR_DIFF%"
            exit 1
          fi

          if (( $(echo "$COVERAGE_DIFF < -2" | bc -l) )); then
            echo "âŒ Test coverage regressed by $COVERAGE_DIFF%"
            exit 1
          fi
        fi

    # Upload artifacts
    - name: Upload Quality Reports
      uses: actions/upload-artifact@v4
      with:
        name: quality-reports
        path: |
          quality-metrics.json
          interface-metrics.json
          quality-dashboard.html

    # Note: Individual PR comments disabled in favor of consolidated CI status
    # The CI Status Consolidator workflow will aggregate all results

    # Final step to ensure workflow status reflects gate results
    - name: Final Quality Gate Status
      if: always()
      run: |
        if [ "${{ steps.gates.outputs.gates_passed }}" != "true" ]; then
          echo "âŒ Quality gates failed"
          exit 1
        else
          echo "âœ… All quality gates passed"
        fi
